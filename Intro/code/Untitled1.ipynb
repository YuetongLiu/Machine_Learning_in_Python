{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "\n",
    "class DecisionStumpEquality:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Get an array with the number of 0's, number of 1's, etc.\n",
    "        count = np.bincount(y)    \n",
    "        \n",
    "        # Get the index of the largest value in count.  \n",
    "        # Thus, y_mode is the mode (most popular value) of y\n",
    "        y_mode = np.argmax(count) \n",
    "\n",
    "        self.splitSat = y_mode\n",
    "        self.splitNot = None\n",
    "        self.splitVariable = None\n",
    "        self.splitValue = None\n",
    "\n",
    "        # If all the labels are the same, no need to split further\n",
    "        if np.unique(y).size <= 1:\n",
    "            return\n",
    "\n",
    "        minError = np.sum(y != y_mode)\n",
    "\n",
    "        # Loop over features looking for the best split\n",
    "        X = np.round(X)\n",
    "\n",
    "        for d in range(D):\n",
    "            for n in range(N):\n",
    "                # Choose value to equate to\n",
    "                value = X[n, d]\n",
    "\n",
    "                # Find most likely class for each split\n",
    "                y_sat = utils.mode(y[X[:,d] == value])\n",
    "                y_not = utils.mode(y[X[:,d] != value])\n",
    "\n",
    "                # Make predictions\n",
    "                y_pred = y_sat * np.ones(N)\n",
    "                y_pred[X[:, d] != value] = y_not\n",
    "\n",
    "                # Compute error\n",
    "                errors = np.sum(y_pred != y)\n",
    "\n",
    "                # Compare to minimum error so far\n",
    "                if errors < minError:\n",
    "                    # This is the lowest error, store this value\n",
    "                    minError = errors\n",
    "                    self.splitVariable = d\n",
    "                    self.splitValue = value\n",
    "                    self.splitSat = y_sat\n",
    "                    self.splitNot = y_not\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        M, D = X.shape\n",
    "        X = np.round(X)\n",
    "\n",
    "        if self.splitVariable is None:\n",
    "            return self.splitSat * np.ones(M)\n",
    "\n",
    "        yhat = np.zeros(M)\n",
    "\n",
    "        for m in range(M):\n",
    "            if X[m, self.splitVariable] == self.splitValue:\n",
    "                yhat[m] = self.splitSat\n",
    "            else:\n",
    "                yhat[m] = self.splitNot\n",
    "\n",
    "        return yhat\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DecisionStumpErrorRate:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" YOUR CODE HERE \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A helper function that computes the entropy of the \n",
    "discrete distribution p (stored in a 1D numpy array).\n",
    "The elements of p should add up to 1.\n",
    "This function ensures lim p-->0 of p log(p) = 0\n",
    "which is mathematically true (you can show this with l'Hopital's rule), \n",
    "but numerically results in NaN because log(0) returns -Inf.\n",
    "\"\"\"\n",
    "def entropy(p):\n",
    "    plogp = 0*p # initialize full of zeros\n",
    "    plogp[p>0] = p[p>0]*np.log(p[p>0]) # only do the computation when p>0\n",
    "    return -np.sum(plogp)\n",
    "    \n",
    "# This is not required, but one way to simplify the code is \n",
    "# to have this class inherit from DecisionStumpErrorRate.\n",
    "# Which methods (init, fit, predict) do you need to overwrite?\n",
    "class DecisionStumpInfoGain(DecisionStumpErrorRate):\n",
    "    pass # DELETE THIS, IMPLEMENT NEW METHOD(S)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStumpErrorRate:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Get an array with the number of 0's, number of 1's, etc.\n",
    "        count = np.bincount(y)    \n",
    "        \n",
    "        # Get the index of the largest value in count.  \n",
    "        # Thus, y_mode is the mode (most popular value) of y\n",
    "        y_mode = np.argmax(count) \n",
    "\n",
    "        self.splitSat = y_mode\n",
    "        self.splitNot = None\n",
    "        self.splitVariable = None\n",
    "        self.splitValue = None\n",
    "\n",
    "        # If all the labels are the same, no need to split further\n",
    "        if np.unique(y).size <= 1:\n",
    "            return\n",
    "\n",
    "        minError = np.sum(y != y_mode)\n",
    "\n",
    "        # Loop over features looking for the best split\n",
    "        X = np.round(X)\n",
    "\n",
    "        for d in range(D):\n",
    "            for n in range(N):\n",
    "                # Choose value to equate to\n",
    "                value = X[n, d]\n",
    "\n",
    "                # Find most likely class for each split\n",
    "                y_sat = utils.mode(y[X[:,d] == value])\n",
    "                y_not = utils.mode(y[X[:,d] != value])\n",
    "\n",
    "                # Make predictions\n",
    "                y_pred = y_sat * np.ones(N)\n",
    "                y_pred[X[:, d] != value] = y_not\n",
    "\n",
    "                # Compute error\n",
    "                errors = np.sum(y_pred != y)\n",
    "\n",
    "                # Compare to minimum error so far\n",
    "                if errors < minError:\n",
    "                    # This is the lowest error, store this value\n",
    "                    minError = errors\n",
    "                    self.splitVariable = d\n",
    "                    self.splitValue = value\n",
    "                    self.splitSat = y_sat\n",
    "                    self.splitNot = y_not\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        M, D = X.shape\n",
    "        X = np.round(X)\n",
    "\n",
    "        if self.splitVariable is None:\n",
    "            return self.splitSat * np.ones(M)\n",
    "\n",
    "        yhat = np.zeros(M)\n",
    "\n",
    "        for m in range(M):\n",
    "            if X[m, self.splitVariable] == self.splitValue:\n",
    "                yhat[m] = self.splitSat\n",
    "            else:\n",
    "                yhat[m] = self.splitNot\n",
    "\n",
    "        return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    plogp = 0*p # initialize full of zeros\n",
    "    plogp[p>0] = p[p>0]*np.log(p[p>0]) # only do the computation when p>0\n",
    "    return -np.sum(plogp)\n",
    "    \n",
    "# This is not required, but one way to simplify the code is \n",
    "# to have this class inherit from DecisionStumpErrorRate.\n",
    "# Which methods (init, fit, predict) do you need to overwrite?\n",
    "class DecisionStumpInfoGain(DecisionStumpErrorRate):\n",
    "    pass # DELETE THIS, IMPLEMENT NEW METHOD(S)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
